{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNFrSzdfcSFX"
      },
      "source": [
        "# **Graded Challenge 7**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huCxQDezcSFZ"
      },
      "source": [
        "# **1. Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTYqrBhocSFa"
      },
      "source": [
        "Name  : Frederick Kurniawan Putra\n",
        "\n",
        "Batch : HCK-016  \n",
        "\n",
        "**Objective**  \n",
        "The Goal of this project is create ann model to analyze sentiment on content in social media to know if the content is hate speech or non hate speech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUbmwalscSFa"
      },
      "source": [
        "# **2. Import Libraries**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h2g0zboav5M",
        "outputId": "57df854c-06a6-4da8-ab2c-854d34f4725b"
      },
      "outputs": [],
      "source": [
        "pip install feature-engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY7iGokRa83u",
        "outputId": "ea7142ab-f6cd-4e7f-9ffe-904b1f622eb1"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scikit-learn joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGGp69mPbEQe",
        "outputId": "706e8fca-c957-4933-8548-49cfae82c9e3"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRAalxUddmp1",
        "outputId": "45979cd0-74e8-4de4-8055-c128a975a3f5"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj8qBJ0tooaA",
        "outputId": "0f41e912-330a-4ecc-fc33-d015612954d0"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "619GiDUOcSFa"
      },
      "outputs": [],
      "source": [
        "# For Data Manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "import seaborn as sns\n",
        "\n",
        "# For Feature Engineering\n",
        "from sklearn.model_selection import train_test_split\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from scipy.stats import kendalltau, spearmanr, pearsonr\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# For Model Definition & Training\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.neighbors import KNeighborsRegressor\n",
        "# from sklearn.svm import SVR\n",
        "# from sklearn.tree import DecisionTreeRegressor\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# For Model Evaluation\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "\n",
        "\n",
        "# For Saving Models\n",
        "import pickle\n",
        "\n",
        "# To Deactivate Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Displaying all columns\n",
        "pd.options.display.max_columns = 200\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import STOPWORDS\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "\n",
        "#paralel processing\n",
        "import multiprocessing as mp\n",
        "\n",
        "import sys\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, GRU, Dropout, Reshape\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.initializers import RandomNormal, HeNormal\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow_hub as tf_hub\n",
        "\n",
        "from sklearn.metrics import ( precision_score, recall_score, f1_score, roc_auc_score,\n",
        "                             classification_report, confusion_matrix)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from imblearn.under_sampling import RandomUnderSampler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1F_JheScSFb"
      },
      "source": [
        "**Import Libraries - Explanation**\n",
        "\n",
        "\n",
        "Before we commence the project, we `import all the necessary libraries required to facilitate various tasks throughout the project`. Each library serves as a tool for specific purposes as indicated by the comments provided above the code. These libraries provide functionalities ranging from data manipulation, visualization, and model building, etc., enabling us to efficiently conduct data analysis and develop machine learning models for our project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_zRt-srcSFb"
      },
      "source": [
        "# **3. Data Loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PtBeX-PcSFb"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oApe_V7ocSFb"
      },
      "outputs": [],
      "source": [
        "#see all columns name\n",
        "pd.set_option('display.max_columns',None)\n",
        "\n",
        "#import data from csv\n",
        "df = pd.read_csv('HateSpeechDatasetTrimmer.csv') # input your file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3a-cYwQcSFc",
        "outputId": "cec44917-525d-4116-8d29-11ad12e7c47e"
      },
      "outputs": [],
      "source": [
        "#Checking data summary\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXRXK0vpcSFc"
      },
      "source": [
        "**Checking Data Summary - Insight**\n",
        "\n",
        "* There are 9999 entries in dataset with no entries with null values.\n",
        "* There are 3 Columns in this dataset. With Content column represents text in social media, and Label that represents Classification of said content.\n",
        "* Content_int column are column that won't be used for this project and will be dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Puomzl6pcSFc",
        "outputId": "4591cecc-f8fb-4718-b28c-2d64f0c226ea"
      },
      "outputs": [],
      "source": [
        "# Identifying Target Column\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiXErrHacSFd"
      },
      "source": [
        "**Import dataset - Explanation**\n",
        "\n",
        "During data loading, first we import dataset from provided csv file and extract all data inside it. All data will be inserted into initial dataFrame `df`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nEVUpl7cSFd"
      },
      "source": [
        "## Drop Content_int column and inputted error data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnWP27xccSFd"
      },
      "outputs": [],
      "source": [
        "df.drop('Content_int',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKrjQyqJcSFd",
        "outputId": "20633d83-a7ad-49b5-8c5a-1342d3450d2f"
      },
      "outputs": [],
      "source": [
        "df[df['Label'] == 'Label'].index.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5Ro2UBlcSFd"
      },
      "outputs": [],
      "source": [
        "df.drop(df[df['Label'] == 'Label'].index.to_list(),inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv2whBTrcSFd"
      },
      "source": [
        "## Change initial data type to appropriate one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X40JCXkucSFe"
      },
      "outputs": [],
      "source": [
        "df['Label'] = df['Label'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzzFeC3_cSFe",
        "outputId": "90a4e5eb-073e-4287-c167-40c545d65d85"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knj66BaXcSFe"
      },
      "source": [
        "## Check Data Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVr-OUcZcSFe",
        "outputId": "dd1a5451-5266-469c-a002-278143524c0c"
      },
      "outputs": [],
      "source": [
        "# Check Data Duplicates\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2y3fWlOcSFe"
      },
      "source": [
        "**Check Data Duplicates - Explanation**\n",
        "\n",
        "The code above helps us find out if there are any duplicate entries in our dataset. When we run it, and the output shows 0, it means `there are no repeated rows or data duplicates in our dataset`. Therefore, we don't need to do anything further to handle duplicated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "qYRYtRbHcSFe",
        "outputId": "f93b1286-416d-4cdb-dfc9-fc020e39bed0"
      },
      "outputs": [],
      "source": [
        "duplicated_text= df[df.duplicated(subset=['Content'], keep=False)].sort_values(by='Content')\n",
        "duplicated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSSdC65GcSFe"
      },
      "outputs": [],
      "source": [
        "# splitted_sentence = df['Content'].str.split()\n",
        "\n",
        "# temp_list = []\n",
        "# temp_list_2 = []\n",
        "\n",
        "# for sentence in splitted_sentence:\n",
        "#     temp_list.append(len(sentence))\n",
        "\n",
        "# for numeric in df['Content_int'].str.split():\n",
        "#     temp_list_2.append(len(numeric))\n",
        "\n",
        "# df_cek = pd.DataFrame({\n",
        "#     \"length_sentence\" : temp_list,\n",
        "#     \"numeric\" : temp_list_2\n",
        "# })\n",
        "\n",
        "# display(df_cek)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53tME3vgcSFf",
        "outputId": "52db370d-2fc3-4194-ca94-d18a86d7d2f6"
      },
      "outputs": [],
      "source": [
        "print(\"Number of Duplicated Rows based on 'Content' column: {}\"\n",
        "      .format(df.duplicated(subset=['Content']).sum()))\n",
        "print(\"Number of Duplicated Rows based on 'Content' and 'Label' columns: {}\"\n",
        "      .format(df.duplicated(subset=['Content', 'Label']).sum()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "qtCFKhfHcSFf",
        "outputId": "cf0ba62d-3cc7-4431-c8b1-6b54dc70a3ae"
      },
      "outputs": [],
      "source": [
        "# Group by 'text' and calculate the number of unique values for 'target'\n",
        "grouped_df_train = df.groupby(['Content'])['Label'].agg(['nunique'])\n",
        "\n",
        "# Filter groups where the number of unique 'target' values is greater than 1\n",
        "df_train_mislabeled = grouped_df_train.query(\"nunique > 1\")\n",
        "\n",
        "print('Total Mislabeled:', df_train_mislabeled.shape[0])\n",
        "df_train_mislabeled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ca84lSZcSFf"
      },
      "outputs": [],
      "source": [
        "# Drop columns where both the text and target have identical values.\n",
        "df.drop_duplicates(subset=['Content', 'Label'],keep='first',inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tRjERLycSFf",
        "outputId": "811df40e-e31a-40ec-8ecc-e9a07a23b565"
      },
      "outputs": [],
      "source": [
        "print(\"Number of Duplicated Rows based on 'Content' column: {}\"\n",
        "      .format(df.duplicated(subset=['Content']).sum()))\n",
        "print(\"Number of Duplicated Rows based on 'Content' and 'Label' columns: {}\"\n",
        "      .format(df.duplicated(subset=['Content', 'Label']).sum()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JCLAZT-ZcSFf",
        "outputId": "412488ce-b967-4b5f-ca97-b93639795f50"
      },
      "outputs": [],
      "source": [
        "duplicated_text= df[df.duplicated(subset=['Content'], keep=False)].sort_values(by='Content')\n",
        "duplicated_text[['Content','Label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRQU3sbGcSFf"
      },
      "outputs": [],
      "source": [
        "#drop all entries that the content has duplicate entries that have other result label\n",
        "duplicate_index = df[df.duplicated(subset=['Content'], keep=False)].index.to_list()\n",
        "df.drop(duplicate_index,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k8qoivdcSFg",
        "outputId": "da46e2a7-4675-4047-dbeb-1371653b83d7"
      },
      "outputs": [],
      "source": [
        "print(\"Number of Duplicated Rows based on 'Content' column: {}\"\n",
        "      .format(df.duplicated(subset=['Content']).sum()))\n",
        "print(\"Number of Duplicated Rows based on 'Content' and 'Label' columns: {}\"\n",
        "      .format(df.duplicated(subset=['Content', 'Label']).sum()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQl_mTZfcSFg",
        "outputId": "127097c0-a308-43e8-f2f3-618c02666c57"
      },
      "outputs": [],
      "source": [
        "# Check Data Duplicates\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P86yfqclcSFg",
        "outputId": "9931bd0f-809a-4451-dfce-f20eeec8d8eb"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zwh8F9HcSFg"
      },
      "source": [
        "## Checking Missing Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr3rUOBIcSFg"
      },
      "outputs": [],
      "source": [
        "def count_misval(df=None, show='both'):\n",
        "\n",
        "    # Calculate the total missing value in each column\n",
        "    missing_values = df.isnull().sum()\n",
        "\n",
        "    # Calculate the percentage of missing values in each column\n",
        "    percentage_missing_values = (missing_values / len(df)) * 100\n",
        "\n",
        "    # Create a new dataframe to store the total and percentage of missing values\n",
        "    missing_df = pd.DataFrame(\n",
        "        {'total missing values': missing_values,\n",
        "         'percentage': percentage_missing_values.round(2)})\n",
        "\n",
        "    #Use this option to select the conditions to be displayed\n",
        "    show_options = {\n",
        "        'misval': missing_df[missing_df['total missing values'] > 0],\n",
        "        'non_misval': missing_df[missing_df['total missing values'] == 0],\n",
        "        'both': missing_df\n",
        "    }\n",
        "\n",
        "    return show_options.get(show, \"Invalid value for 'show' parameter. Use 'both', 'misval', or 'non_misval'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "sdQPwvntcSFh",
        "outputId": "9dfbc391-eb36-4423-f59d-bf43e572b96c"
      },
      "outputs": [],
      "source": [
        "count_misval(df=df,show='both')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZWmhD3jcSFh"
      },
      "source": [
        "# **4. Exploratory Data Analysis**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnQApepcgYQ6",
        "outputId": "c0478a37-bc9d-4ab5-a795-778244c9444e"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E9RRLwfcSFh"
      },
      "source": [
        "## a. Distribution of Words in Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6SSJHxRcSFh",
        "outputId": "35433e8f-91e0-4677-9dce-e41993e3fea8"
      },
      "outputs": [],
      "source": [
        "# show length of stopwords list from nltk stopwords\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "print('length of stopword using nltk:',len(nltk_stopwords))\n",
        "\n",
        "#show length of stopwords list from wordcloud\n",
        "print('length of stopword using wordcloud:',len(STOPWORDS))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyN8zSC4cSFi"
      },
      "outputs": [],
      "source": [
        "#Convert text to lowercase\n",
        "df['Content']= df['Content'].apply(lambda row: str(row).lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn3OvE24cSFi"
      },
      "outputs": [],
      "source": [
        "def calculate_word_metrics(df, col: str = None):\n",
        "\n",
        "    # Convert the text in the specified column to lowercase\n",
        "    df[col] = df[col].apply(lambda row: str(row).lower())\n",
        "\n",
        "    # Calculate word metrics\n",
        "    word_count = df[col].apply(lambda row: len(str(row).split()))\n",
        "    unique_word_count = df[col].apply(lambda row: len(set(str(row).split())))\n",
        "    stop_word_count = df[col].apply(lambda row: len([w for w in str(row).split() if w in STOPWORDS]))\n",
        "\n",
        "    df_word_metrics= pd.DataFrame({\n",
        "        'word_count': word_count,\n",
        "        'unique_word_count': unique_word_count,\n",
        "        'stop_word_count': stop_word_count,\n",
        "    })\n",
        "\n",
        "    return df_word_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "n_d4oYC0cSFi",
        "outputId": "b9abd74a-b53b-4af1-d6df-5e5644980f5e"
      },
      "outputs": [],
      "source": [
        "df_metrics= calculate_word_metrics(df, col='Content')\n",
        "\n",
        "\n",
        "df_metrics.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0v-2UAfcSFi"
      },
      "outputs": [],
      "source": [
        "def additional_metrics(df, col: str = None):\n",
        "\n",
        "    # Calculate additional text metrics\n",
        "    str_punctuation= '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "    char_count = df[col].apply(lambda x: len(str(x)))\n",
        "    punctuation_count = df[col].apply(lambda x: len([c for c in str(x) if c in str_punctuation]))\n",
        "    hashtag_count = df[col].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "    mention_count = df[col].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
        "    url_count = df[col].apply(lambda x: len([w for w in str(x).split() if 'http' in w or 'https' in w]))\n",
        "\n",
        "    # Create a new DataFrame with all calculated metrics\n",
        "    df_additional_metrics = pd.DataFrame({\n",
        "        'char_count': char_count,\n",
        "        'punctuation_count': punctuation_count,\n",
        "        'hashtag_count': hashtag_count,\n",
        "        'mention_count': mention_count,\n",
        "        'url_count': url_count\n",
        "    })\n",
        "\n",
        "    return df_additional_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "xtOp-_vXcSFj",
        "outputId": "2853d8fb-7252-4e9f-e261-7c0cebcc161c"
      },
      "outputs": [],
      "source": [
        "additional_metrics(df, col='Content').head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yzLYH-gcSFj"
      },
      "outputs": [],
      "source": [
        "#Concatenate df_metrics with additional metrics\n",
        "df_metrics= pd.concat([df_metrics, additional_metrics(df, col='Content')],\n",
        "                            axis = 1,join='inner')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H7Y_enecSFj",
        "outputId": "cfe257cb-a9aa-4df8-82e7-13a91674ea92"
      },
      "outputs": [],
      "source": [
        "#show result from previous concatenate\n",
        "print(df_metrics.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZjVuAZocSFj",
        "outputId": "f68c6e58-8bb7-49d7-f2f3-7bfa80367f59"
      },
      "outputs": [],
      "source": [
        "#create new dataframe with contatenating original dataframe with df_metrics\n",
        "df_new= pd.concat([df, df_metrics], axis = 1,join='inner')\n",
        "\n",
        "#showing the result\n",
        "print(df_new.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqUuIhHMcSFj"
      },
      "outputs": [],
      "source": [
        "def subplot_hist(df=None, columns=None, hue=None,\n",
        "                 nrows=1, ncols=2, suptitle=None, palette=None):\n",
        "\n",
        "    nrows = len(columns) // 2 + len(columns) % 2\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(10, 8))\n",
        "\n",
        "    # Loop through each column and create a subplot\n",
        "    for i, column in enumerate(columns):\n",
        "        sns.histplot(data=df, x=column, hue=hue, bins=20,\n",
        "                     kde=True, ax=axes[i//2, i%2], palette=palette)\n",
        "\n",
        "        axes[i//2, i%2].set_title(f'{column} distribution base on target', size=10)  # Set title with column name\n",
        "\n",
        "    # Check if the number of columns is odd\n",
        "    if len(columns) % 2 == 1:\n",
        "        # Remove the empty subplot in the last row if the number of columns is odd\n",
        "        fig.delaxes(axes[-1, -1])\n",
        "\n",
        "    # Add super title\n",
        "    if suptitle:\n",
        "        plt.suptitle(suptitle, fontsize=14)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plots\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "zC6tuz26cSFj",
        "outputId": "d5ebe733-4f17-4d6b-9eb9-2f3b72374d11"
      },
      "outputs": [],
      "source": [
        "#choose column that has unique value more than 1\n",
        "columns_to_plot = []\n",
        "for column in df_new.columns[2:]:\n",
        "    if df_new[column].nunique() > 1:\n",
        "        columns_to_plot.append(column)\n",
        "\n",
        "#show subplot\n",
        "subplot_hist(df_new,columns= columns_to_plot,\n",
        "             hue='Label', suptitle='Distribution of Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "FMf_J2BzcSFj",
        "outputId": "1e9c1f3b-5e6b-470c-dbaf-46839d1c4343"
      },
      "outputs": [],
      "source": [
        "df_new[columns_to_plot].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssHMK-wpcSFj"
      },
      "source": [
        "## b. Percentage of Hate Speech and Non Hate Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3csXgwBlcSFj"
      },
      "outputs": [],
      "source": [
        "def generate_ngrams(text, n_gram=1):\n",
        "    # Tokenize the text, remove empty strings and stopwords\n",
        "    token = [token for token in text.split(' ') if token != '' if token not in STOPWORDS]\n",
        "\n",
        "    # Generate n-grams using a sliding window approach\n",
        "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
        "\n",
        "    # Combine the n-grams into a list of strings\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Bc3domP-cSFk",
        "outputId": "82ab8ac6-6ddc-4883-a258-b4eff8e4a942"
      },
      "outputs": [],
      "source": [
        "df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtlJ2oOycSFk",
        "outputId": "b5dd3c45-b237-4d49-c970-90a6ef0563da"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OwoqGy4cSFk"
      },
      "outputs": [],
      "source": [
        "content_label1= df_new.query(\"Label==1\")['Content']\n",
        "content_label0= df_new.query(\"Label==0\")['Content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H7Ltvb42cSFk",
        "outputId": "5963e6a8-a233-4365-acdb-626661655992"
      },
      "outputs": [],
      "source": [
        "content_label1[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv7cX2SLcSFk",
        "outputId": "330c539f-f687-4d7c-9321-74cf4fe0cc10"
      },
      "outputs": [],
      "source": [
        "generate_ngrams(content_label1[0], n_gram=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96sy0GoVcSFk"
      },
      "outputs": [],
      "source": [
        "def ngrams_frequencies(text_data, n_grams=1, name='unigram'):\n",
        "    \"\"\"\n",
        "    This function creates a DataFrame containing n-grams frequencies based on the provided text data.\n",
        "\n",
        "    Parameters:\n",
        "    - text_data: Iterable of text data\n",
        "    - n_grams: Size of n-grams (default is 1 for unigrams)\n",
        "    - name: Name to use for the n-grams column in the DataFrame (default is 'unigram')\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: DataFrame containing n-gram frequencies\n",
        "    \"\"\"\n",
        "    # Initialize an empty dictionary to store word frequencies\n",
        "    ngrams_dict = {}\n",
        "\n",
        "    # Iterate over each text in the provided text data\n",
        "    for text in text_data:\n",
        "        # Generate n-grams for each word in the text\n",
        "        for ngram in generate_ngrams(text, n_grams):\n",
        "            # Update the frequency count for each n-gram in the word_dict dictionary\n",
        "            ngrams_dict[ngram] = ngrams_dict.get(ngram, 0) + 1\n",
        "\n",
        "    # Convert the n-gram dictionary to a DataFrame\n",
        "    df_ngrams = pd.DataFrame(list(ngrams_dict.items()), columns=[name, f'{name}_counts'])\n",
        "\n",
        "    return df_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo9-0maHcSFk"
      },
      "outputs": [],
      "source": [
        "df_hate_speech_unigrams= ngrams_frequencies(content_label1) #default n_grams=1, name='unigram'\n",
        "df_not_hate_speech_unigrams= ngrams_frequencies(content_label0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJT8Aj8rcSFl"
      },
      "outputs": [],
      "source": [
        "df_hate_speech_unigrams=df_hate_speech_unigrams.sort_values(by='unigram_counts', ascending=False)\n",
        "df_not_hate_speech_unigrams=df_not_hate_speech_unigrams.sort_values(by='unigram_counts', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "VgPb7cOJcSFl",
        "outputId": "45d82a8e-b2c4-4d76-8ac7-342060abaa91"
      },
      "outputs": [],
      "source": [
        "df_hate_speech_unigrams.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9WI2iKJcSFl"
      },
      "outputs": [],
      "source": [
        "def custom_barplot(ax,data,x, y, top,title,color, title_size=18, fontsize=14):\n",
        "    \"\"\"\n",
        "    Create a bar plot on the given axes.\n",
        "\n",
        "    Parameters:\n",
        "    - ax: Axes object to create the bar plot on\n",
        "    - data: DataFrame containing the data for the bar plot\n",
        "    - x: Column representing the x-axis values in the DataFrame\n",
        "    - y: Column representing the y-axis values in the DataFrame\n",
        "    - top (int): The number of top items to be displayed in the bar plot\n",
        "    - title: Title for the bar plot\n",
        "    - color: Color for the bars\n",
        "    - title_size (int): Font size for the title (default is 20)\n",
        "    - fontsize (int): Font size for other elements like axis labels (default is 14)\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    bplot= sns.barplot(data=data.head(top),\n",
        "                x=x, y=y,\n",
        "                orient='h', ax=ax, color=color)\n",
        "\n",
        "    ax.set_title(title, size=title_size)\n",
        "    ax.set_ylabel('')\n",
        "\n",
        "    # Set font size for y-axis ticks\n",
        "    ax.tick_params(axis='y', labelsize=fontsize)\n",
        "\n",
        "    for i in bplot.containers:\n",
        "        bplot.bar_label(i, fontsize=fontsize+2, label_type='center',color='white')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "UazKKa5mcSFl",
        "outputId": "bbe4a9d7-a427-4bdf-863f-2134da6717a2"
      },
      "outputs": [],
      "source": [
        "#setting of figsize & subplot\n",
        "fig = plt.figure(figsize = (24,12))\n",
        "\n",
        "gs = fig.add_gridspec(10,24)\n",
        "gs.update(wspace = 0.5, hspace = 0.05)\n",
        "\n",
        "\n",
        "ax1 = fig.add_subplot(gs[1:12, 0:15]) #pie chart position\n",
        "ax2 = fig.add_subplot(gs[1:5,12:]) #text location\n",
        "ax3 = fig.add_subplot(gs[4:12,13:18]) #bar chart\n",
        "ax4 = fig.add_subplot(gs[4:12,18:23]) #bar chart\n",
        "\n",
        "# axes list\n",
        "axes = [ax1,ax2,ax3,ax4]\n",
        "\n",
        "# setting of axes; visibility of axes and spines turn off\n",
        "for ax in axes:\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(True)\n",
        "    #ax.set_facecolor('#f6f5f5')\n",
        "\n",
        "    for loc in ['left', 'right', 'top', 'bottom']:\n",
        "        ax.spines[loc].set_visible(False)\n",
        "\n",
        "ax2.axes.get_xaxis().set_visible(False)\n",
        "ax2.axes.get_yaxis().set_visible(False)\n",
        "\n",
        "# ax1 (Pie Chart)\n",
        "labels = ['Hate Speech', 'Non Hate Speech']\n",
        "val_counts = df_new['Label'].value_counts()\n",
        "# colors = colors_1[2:4]\n",
        "explode = [0.08, 0]\n",
        "\n",
        "def autopct_format(pct):\n",
        "    absolute = int(pct/100.*sum(val_counts))\n",
        "    return f'{pct:.2f}%\\n({absolute} {labels[0] if pct < 50 else labels[1]})'\n",
        "\n",
        "patches, texts, autotexts = ax1.pie(val_counts,\n",
        "                                    explode=explode, shadow=True,\n",
        "                                    startangle=90, autopct=autopct_format,\n",
        "                                    textprops={'color': \"black\", 'weight': 'bold', 'alpha': 1, 'size': 15},\n",
        "                                    wedgeprops={\"edgecolor\": \"black\", 'linewidth': 2, 'antialiased': True},\n",
        "                                    pctdistance=0.5)\n",
        "\n",
        "# Change the color of the first text\n",
        "autotexts[0].set_color('w')\n",
        "\n",
        "# # ax2 (Text)\n",
        "# ax2.text(0.01, 0.8, 'Analysis of Disaster and Non-Disaster Tweets',\n",
        "#          {'family': 'Serif', 'size': '25', 'weight': 'bold', 'color': 'black'},\n",
        "#          bbox=dict(boxstyle='round', facecolor='lightgrey', edgecolor='none'))\n",
        "\n",
        "# text = '''57% of the tweets are related to disasters, whereas 43% do not pertain to disasters.\n",
        "# We can observe the distribution of words for each class based on unigram analysis.\n",
        "# However, this dataset is not clean yet.'''\n",
        "# ax2.text(0.01, 0.45, text, {'family': 'Serif', 'size': '15', 'color': 'black'})\n",
        "\n",
        "\n",
        "# ax3 (barplot)\n",
        "\n",
        "custom_barplot(ax3, df_hate_speech_unigrams,'unigram_counts','unigram', 10,\n",
        "               f'Top 10 Most Frequent Unigrams \\nin Hate Content','red',\n",
        "               title_size=12,fontsize=10)\n",
        "\n",
        "# ax4 (barplot)\n",
        "custom_barplot(ax4, df_not_hate_speech_unigrams,'unigram_counts','unigram', 10,\n",
        "               f'Top 10 Most Frequent Unigrams \\nin Non-Hate Content','green',\n",
        "               title_size=12,fontsize=10)\n",
        "\n",
        "# invert x axis\n",
        "ax4.invert_xaxis()\n",
        "ax4.set_ylabel('')\n",
        "ax4.yaxis.tick_right()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQHCZO-7JVkU"
      },
      "source": [
        "**Explanation**\n",
        "\n",
        "Here we can see that our dataset has 85.58% Non Hate Speech, while hate speech represents 14.42% data. This means that the data is imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "HJjEGfNOcSFl",
        "outputId": "c047d200-9997-469f-a402-cbf354f8ae9c"
      },
      "outputs": [],
      "source": [
        "# figsize & subplot\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "gs = fig.add_gridspec(12, 20)\n",
        "gs.update(wspace=0.5, hspace=0.05)\n",
        "\n",
        "ax1 = fig.add_subplot(gs[1:12, :9])  # bar chart (disaster tweets)\n",
        "ax2 = fig.add_subplot(gs[1:12, 10:])  # bar chart (non-disaster tweets)\n",
        "\n",
        "# axes list\n",
        "axes = [ax1, ax2]\n",
        "\n",
        "# setting of axes; visibility of axes and spines turn off\n",
        "for ax in axes:\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(True)\n",
        "\n",
        "    for loc in ['left', 'right', 'top', 'bottom']:\n",
        "        ax.spines[loc].set_visible(False)\n",
        "\n",
        "# Create bar plot for ax1\n",
        "custom_barplot(ax1, df_hate_speech_unigrams,'unigram_counts','unigram', 20,\n",
        "               f'Top 20 Most Frequent Unigrams \\nin Hate Speech Contents','red')\n",
        "\n",
        "# Create bar plot for ax2\n",
        "custom_barplot(ax2, df_not_hate_speech_unigrams,'unigram_counts','unigram', 20,\n",
        "               f'Top 20 Most Frequent Unigrams \\nin Non Hate Speech Contents','green')\n",
        "\n",
        "# Invert x-axis for ax2\n",
        "ax2.invert_xaxis()\n",
        "ax2.set_ylabel('')\n",
        "ax2.yaxis.tick_right()\n",
        "\n",
        "# Add a common title for the entire subplot\n",
        "fig.suptitle(\"Comparison of Unigrams in Hate Speech and Non Hate Speech Content\", size=23)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD7tDV-dKajX"
      },
      "source": [
        "**Explanation**\n",
        "\n",
        "In this data we can see that Most frequent Unigrams that might represent mockery and racism are white, black, slut, afro, faggot. This indicates that racism slur is a indication of hate speech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG1EoOTMcSFl"
      },
      "source": [
        "## c. Top Bigram in Hate Speech and Non Hate Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq9IDT9ucSFl"
      },
      "outputs": [],
      "source": [
        "# Calculate bigram frequencies for disaster and non-disaster tweets\n",
        "df_hate_speech_bigrams= ngrams_frequencies(content_label1,n_grams=2, name='bigram')\n",
        "df_non_hate_speech_bigrams= ngrams_frequencies(content_label0, n_grams=2, name='bigram')\n",
        "\n",
        "# Sort the DataFrames based on bigram counts\n",
        "df_hate_speech_bigrams=df_hate_speech_bigrams.sort_values(by='bigram_counts', ascending=False)\n",
        "df_non_hate_speech_bigrams=df_non_hate_speech_bigrams.sort_values(by='bigram_counts', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "iFvAmax3cSFl",
        "outputId": "3ad82d40-c3bf-4820-e720-9b3114a496fe"
      },
      "outputs": [],
      "source": [
        "# figsize & subplot\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 12),\n",
        "                         gridspec_kw={'wspace': 0.5, 'hspace': 0.05})\n",
        "\n",
        "# axes list\n",
        "ax1, ax2 = axes\n",
        "\n",
        "# setting of axes; visibility of axes and spines turn off\n",
        "for ax in axes:\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(True)\n",
        "\n",
        "    for loc in ['left', 'right', 'top', 'bottom']:\n",
        "        ax.spines[loc].set_visible(False)\n",
        "\n",
        "# Create bar plot for ax1\n",
        "custom_barplot(ax1, df_hate_speech_bigrams, 'bigram_counts', 'bigram', 20,\n",
        "               'Top 20 Most Frequent Bigrams in Hate Speech Content', 'red')\n",
        "\n",
        "# Create bar plot for ax2\n",
        "custom_barplot(ax2, df_non_hate_speech_bigrams, 'bigram_counts', 'bigram', 20,\n",
        "               'Top 20 Most Frequent Bigrams in Non-Hate Speech Content', 'green')\n",
        "\n",
        "# Invert x-axis for ax2\n",
        "ax2.invert_xaxis()\n",
        "ax2.set_ylabel('')\n",
        "ax2.yaxis.tick_right()\n",
        "\n",
        "# Add a common title for the entire subplot\n",
        "fig.suptitle(\"Comparison of Bigrams in Hate Speech and Non-Hate Speech Tweets\", size=23)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHh6sobXKzi0"
      },
      "source": [
        "**Explanation**\n",
        "\n",
        "Here we can see that Most Frequent Bigrams in hate speech content also dominated with racial slur such as afro american, ching chong, non whites, while also followed by mockery such as shithole countries and fucking retard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEe6F7Q-cSFl"
      },
      "source": [
        "## d. Top Country Topic in Hate Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxobw8_5cSFl"
      },
      "outputs": [],
      "source": [
        "# Loads spaCy models for NER and POS (Part-of-Speech)\n",
        "language_model = spacy.load('en_core_web_sm')\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self, model_name=\"en_core_web_sm\"):\n",
        "        self.language_model = spacy.load(model_name)\n",
        "\n",
        "    def apply_ner(self, text):\n",
        "        doc = self.language_model(text)\n",
        "        entities = [ent.text for ent in doc.ents if ent.label_ in ['PERSON']]\n",
        "        return ' '.join(entities)\n",
        "\n",
        "    def apply_pos(self, text):\n",
        "        doc = self.language_model(text)\n",
        "        pos_filtered = [token.text for token in doc if token.pos_ in ['NOUN', 'VERB']]\n",
        "        return ' '.join(pos_filtered)\n",
        "\n",
        "    def clean_text_from_entities(self, text):\n",
        "        doc = self.language_model(text)\n",
        "        entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "        # Menghapus entitas dari teks\n",
        "        for entity in entities:\n",
        "            text = text.replace(entity, '')\n",
        "        return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg8O2f_KcSFl",
        "outputId": "ac3d7df5-4f7c-4c5e-e67c-a656372ab972"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from defs import process_chunk\n",
        "\n",
        "# Function to process each chunk of the DataFrame\n",
        "\n",
        "\n",
        "# Divide the DataFrame into several chunks\n",
        "# The number of chunks is equal to the number of CPU cores\n",
        "total_cpu = mp.cpu_count()\n",
        "chunk_size = len(df_new) // total_cpu\n",
        "print(f'{chunk_size=}')\n",
        "print(f'Total CPU cores: {total_cpu}')\n",
        "\n",
        "chunks = [df_new.iloc[i:i + chunk_size]\n",
        "          for i in range(0, len(df_new), chunk_size)]\n",
        "\n",
        "#  Process each chunk in parallel using multiprocessing\n",
        "with mp.Pool(mp.cpu_count()) as pool:\n",
        "    processed_chunks = pool.map(process_chunk, chunks)\n",
        "\n",
        "# Concatenate the processed chunks to get the final DataFrame\n",
        "df_new = pd.concat(processed_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "IaGfDc67cSFm",
        "outputId": "3b891e9c-5279-4b2a-9bac-5aa9ee88594e"
      },
      "outputs": [],
      "source": [
        "df_new.query(\"Label==1\").iloc[:,-2:].tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6362z0wcSFm"
      },
      "outputs": [],
      "source": [
        "df_new['ner_text'] = df_new['ner_text'].replace('', 'unknown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKMSc9BwcSFm",
        "outputId": "e387e237-ac04-45d5-820d-3defab928851"
      },
      "outputs": [],
      "source": [
        "hate_speech_target_location= df_new.query(\"Label==1\")\n",
        "hate_speech_target_location.ner_text.value_counts().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JXR9-JlcSFm",
        "outputId": "4a883e4f-badf-4885-ee93-268216ecfd14"
      },
      "outputs": [],
      "source": [
        "hate_speech_target_location = hate_speech_target_location.query(\"ner_text != 'unknown'\")\n",
        "hate_speech_target_location.ner_text.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "vJ7IFhX7cSFm",
        "outputId": "8e4cfd50-50ba-448e-9105-058dfa05352f"
      },
      "outputs": [],
      "source": [
        "combined_text = ' '.join(hate_speech_target_location['ner_text'])\n",
        "\n",
        "# Membuat objek WordCloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsUfNjigLQUk"
      },
      "source": [
        "**Explanation**\n",
        "\n",
        "Here we can see that country that become a topic or target for hate speeech are africa, america, europt or london. We can safely assume based on bigram analysis that the context is refering to african american."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3z72W7pcSFm"
      },
      "outputs": [],
      "source": [
        "# Split values in the 'ner_text' column & remove duplicate entities.\n",
        "df_new['ner_text'] = df_new['ner_text'].apply(lambda x: ' '.join(set(x.split())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGZ2RzpwcSFm",
        "outputId": "78141554-f680-423b-fcad-52ce5b42be1e"
      },
      "outputs": [],
      "source": [
        "hate_speech_target_location = df_new.query(\"ner_text != 'unknown' and Label==1\")\n",
        "top10_target = hate_speech_target_location.ner_text.value_counts().head(10)\n",
        "top10_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "QakDZVQicSFm",
        "outputId": "944064ed-8a67-4d32-afc0-69bafb3e333f"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.axes.get_xaxis().set_visible(False)\n",
        "ax.axes.get_yaxis().set_visible(True)\n",
        "\n",
        "for loc in ['left', 'right', 'top', 'bottom']:\n",
        "    ax.spines[loc].set_visible(False)\n",
        "\n",
        "# Use the custom_barplot function\n",
        "custom_barplot(ax, data=top10_target.reset_index(), x='count', y='ner_text', top=10,\n",
        "               title='Top 10 Hate Speech Target',color='red')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvA0thiwcSFm"
      },
      "source": [
        "# 5. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8r1yb0acSFm"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpMRIxWDcSFm"
      },
      "outputs": [],
      "source": [
        "def remove_stopword(text, words_to_remove=None):\n",
        "    # Add words_to_remove to STOPWORDS\n",
        "    if words_to_remove:\n",
        "        STOPWORDS.update(words_to_remove)\n",
        "\n",
        "    # Tokenize the text by splitting it. Remove empty strings and stopwords\n",
        "    tokens = [token for token in re.split(r'\\s+', text) if token != '' if token not in STOPWORDS]\n",
        "\n",
        "    # Join tokens back into text\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z4aRzp4cSFm"
      },
      "outputs": [],
      "source": [
        "def apply_stemming(text):\n",
        "    \"\"\"\n",
        "    This function applies stemming to the input text using Porter Stemmer.\n",
        "\n",
        "    Parameters:\n",
        "    - text: Input text\n",
        "\n",
        "    Returns:\n",
        "    - stemmed_text: Text after stemming\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in re.split(r'\\s+', text) if token != '']\n",
        "    stemmed_text = ' '.join(tokens)\n",
        "    return stemmed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgz0mLFQcSFm"
      },
      "outputs": [],
      "source": [
        "def clean_text(text, words_to_remove=None, use_stemmer=False):\n",
        "    \"\"\"\n",
        "    This function cleans text by removing special characters, converting text to lowercase,\n",
        "    and performing additional cleaning steps as needed.\n",
        "\n",
        "    Parameters:\n",
        "    - text: Text to be cleaned\n",
        "    - words_to_remove: List of words to be removed, e.g., ['will', 'one']\n",
        "    - use_stemmer: Boolean, whether to use stemming or not\n",
        "\n",
        "    Returns:\n",
        "    - cleaned_text: The text after being cleaned\n",
        "    \"\"\"\n",
        "\n",
        "    text = text.lower() # Convert text to lowercase\n",
        "    text = remove_stopword(text,words_to_remove)#remove stopword\n",
        "    text = re.sub(r'&\\w+;', ' ', text) # Remove HTML entities like &amp;\n",
        "    text = re.sub(r'@\\S+', '', text) # Remove mentions (Twitter handles) starting with @\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text) # Remove URLs\n",
        "    text = re.sub(r'\\d', '', text) # Remove digits (numeric characters)\n",
        "\n",
        "    # Remove non-alphanumeric characters (except spaces)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Apply stemming if specified\n",
        "    if use_stemmer:\n",
        "        text= apply_stemming(text)\n",
        "\n",
        "    # Replace multiple spaces with a single space and strip leading/trailing spaces\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def clean_text_df(df, col_name, words_to_remove=None, use_stemmer=False):\n",
        "    \"\"\"\n",
        "    This function cleans the text column in a dataframe using the clean_text function.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to be cleaned\n",
        "    - col_name: Name of the text column in the dataframe\n",
        "    - words_to_remove: List of words to be removed, e.g., ['will', 'one', 'body']\n",
        "    - use_stemmer: Boolean, whether to use stemming or not\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with the cleaned text column\n",
        "    \"\"\"\n",
        "    df_cleaned = df.copy()\n",
        "    df_cleaned[col_name] = df_cleaned[col_name].apply(lambda x: clean_text(x, words_to_remove, use_stemmer))\n",
        "    return df_cleaned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocBpr1LWcSFm"
      },
      "source": [
        "### Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeUMnI62cSFn"
      },
      "outputs": [],
      "source": [
        "sys.setrecursionlimit(6000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQol4aPQcSFn"
      },
      "outputs": [],
      "source": [
        "df_cleaned = clean_text_df(df, 'Content', use_stemmer=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation**\n",
        "\n",
        "Here we clean sentence from stopwords, mention, and html that might influence the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "yqdCWRGncSFn",
        "outputId": "d721b3c7-074c-41b1-e536-4ffdd52374bd"
      },
      "outputs": [],
      "source": [
        "calculate_word_metrics(df_cleaned, col='Content').describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "f4hStBFGcSFn",
        "outputId": "9c98d87a-2f2a-43f4-89cd-538d86f442a0"
      },
      "outputs": [],
      "source": [
        "additional_metrics(df_cleaned, col='Content').describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNqOdXQvcSFn"
      },
      "outputs": [],
      "source": [
        "# Extract text data for disaster and non-disaster tweets\n",
        "text_target1= df_cleaned.query(\"Label==1\")['Content']\n",
        "text_target0= df_cleaned.query(\"Label==0\")['Content']\n",
        "\n",
        "# Calculate bigram frequencies for disaster and non-disaster tweets\n",
        "df_hate_speech_target_bigrams= ngrams_frequencies(text_target1,n_grams=2, name='bigram')\n",
        "df_non_hate_speech_target_bigrams= ngrams_frequencies(text_target0, n_grams=2, name='bigram')\n",
        "\n",
        "# Sort the DataFrames based on bigram counts\n",
        "df_hate_speech_target_bigrams=df_hate_speech_target_bigrams.sort_values(by='bigram_counts', ascending=False)\n",
        "df_non_hate_speech_target_bigrams=df_non_hate_speech_target_bigrams.sort_values(by='bigram_counts', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "Z6-PlxglcSFn",
        "outputId": "5cdb2dcf-4eaf-4447-8c25-fc587857ef9b"
      },
      "outputs": [],
      "source": [
        "# figsize & subplot\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 12),\n",
        "                         gridspec_kw={'wspace': 0.5, 'hspace': 0.05})\n",
        "\n",
        "# axes list\n",
        "ax1, ax2 = axes\n",
        "\n",
        "# setting of axes; visibility of axes and spines turn off\n",
        "for ax in axes:\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(True)\n",
        "\n",
        "    for loc in ['left', 'right', 'top', 'bottom']:\n",
        "        ax.spines[loc].set_visible(False)\n",
        "\n",
        "# Create bar plot for ax1\n",
        "custom_barplot(ax1, df_hate_speech_target_bigrams, 'bigram_counts', 'bigram', 20,\n",
        "               'Top 20 Most Frequent Bigrams in Hate Speech Content', 'red')\n",
        "\n",
        "# Create bar plot for ax2\n",
        "custom_barplot(ax2, df_non_hate_speech_target_bigrams, 'bigram_counts', 'bigram', 20,\n",
        "               'Top 20 Most Frequent Bigrams in Non Hate Speech Content', 'green')\n",
        "\n",
        "# Invert x-axis for ax2\n",
        "ax2.invert_xaxis()\n",
        "ax2.set_ylabel('')\n",
        "ax2.yaxis.tick_right()\n",
        "\n",
        "# Add a common title for the entire subplot\n",
        "fig.suptitle(\"Comparison of Bigrams in Hate Speech and Non Hate Speech Content\", size=23)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnCVN1kAcSFn"
      },
      "source": [
        "## Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEVbW4sGcSFn",
        "outputId": "e57d3f7d-b574-412b-d77c-b1af91783ca4"
      },
      "outputs": [],
      "source": [
        "df_cleaned.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMJOTdIYyKCw",
        "outputId": "e95e2e08-9def-4a32-eb44-b769d30d4778"
      },
      "outputs": [],
      "source": [
        "df_cleaned['Content'].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-Q6O6RLcSFn",
        "outputId": "a1a8aa26-12bb-489e-c944-5535e494242d"
      },
      "outputs": [],
      "source": [
        "# Data Splitting\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(df_cleaned.Content,\n",
        "                                                    df_cleaned.Label,\n",
        "                                                    test_size=0.15,\n",
        "                                                    random_state=20,\n",
        "                                                    stratify=df_cleaned.Label)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val,\n",
        "                                                  y_train_val,\n",
        "                                                  test_size=0.10,\n",
        "                                                  random_state=20,\n",
        "                                                  stratify=y_train_val)\n",
        "\n",
        "print('Train Size : ', X_train.shape)\n",
        "print('Val Size   : ', X_val.shape)\n",
        "print('Test Size  : ', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sISuzS0zcSFn"
      },
      "source": [
        "# 6. ANN Training (Sequential API/Functional API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q30ISp9kcSFn"
      },
      "outputs": [],
      "source": [
        "# Tokenization using CountVectorizer\n",
        "count_vec = CountVectorizer()\n",
        "X_train_vec = count_vec.fit_transform(X_train)\n",
        "X_val_vec = count_vec.transform(X_val)\n",
        "X_test_vec = count_vec.transform(X_test)\n",
        "\n",
        "\n",
        "# # #If we want to use tf.dataset, we need to convert it into an array.\n",
        "# X_train_vec= X_train_vec.toarray()\n",
        "# X_val_vec= X_val_vec.toarray()\n",
        "# X_test_vec= X_test_vec.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_UnlDs0cSFn",
        "outputId": "b2a66aff-7627-4037-b03a-b708800736cc"
      },
      "outputs": [],
      "source": [
        "total_vocab = len(count_vec.vocabulary_.keys())\n",
        "max_sen_len = max([len(i.split(\" \")) for i in X_train])\n",
        "# total_samples= len(X_train_vec)\n",
        "\n",
        "print('Total Vocab : ', total_vocab)\n",
        "print('Maximum Sentence Length : ', max_sen_len, 'tokens')\n",
        "# print('Total Samples (rows):', total_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg-LRNzLcSFn"
      },
      "outputs": [],
      "source": [
        "# Text Vectorization\n",
        "\n",
        "\n",
        "\n",
        "text_vectorization = TextVectorization(max_tokens=20000,\n",
        "                                       standardize=\"lower_and_strip_punctuation\",\n",
        "                                       split=\"whitespace\",\n",
        "                                       ngrams=None,\n",
        "                                       output_mode=\"int\",\n",
        "                                       output_sequence_length=max_sen_len,\n",
        "                                       input_shape=(1,)) # Only use in Sequential API\n",
        "\n",
        "text_vectorization.adapt(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP9LaFG_cSFo",
        "outputId": "3a0a11ca-ee30-4bea-9155-6520c5799830"
      },
      "outputs": [],
      "source": [
        "# Example Result\n",
        "\n",
        "## Document example\n",
        "print('Document example')\n",
        "print(df_cleaned.Content[0])\n",
        "print('')\n",
        "\n",
        "## Result of Text Vectorization\n",
        "print('Result of Text Vectorization')\n",
        "print(text_vectorization([df_cleaned.Content[0]]))\n",
        "print('Vector size : ', text_vectorization([df_cleaned.Content[0]]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DrtTB07cSFo",
        "outputId": "b4f832ab-bafd-4f4c-838b-d03df1fc9cd7"
      },
      "outputs": [],
      "source": [
        "# View the Top 20 Tokens (Sorted by the Highest Frequency of Appearance)\n",
        "\n",
        "text_vectorization.get_vocabulary()[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2nvi_TacSFo"
      },
      "outputs": [],
      "source": [
        "# Embedding\n",
        "\n",
        "\n",
        "embedding = Embedding(input_dim=20000,\n",
        "                      output_dim=128,\n",
        "                      embeddings_initializer=\"uniform\",\n",
        "                      input_length=max_sen_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW0SaWbycSFo",
        "outputId": "d17f046d-e38d-4f98-acf4-e28c9dd6c503"
      },
      "outputs": [],
      "source": [
        "#Example Result\n",
        "\n",
        "## Document example\n",
        "print('Document example')\n",
        "print(df_cleaned.Content[0])\n",
        "print('')\n",
        "\n",
        "## Result of Text Vectorization\n",
        "print('Result of  Text Vectorization')\n",
        "print(text_vectorization([df_cleaned.Content[0]]))\n",
        "print('Vector size : ', text_vectorization([df_cleaned.Content[0]]).shape)\n",
        "print('')\n",
        "\n",
        "## Result of  Embedding\n",
        "print('Result of  Embedding')\n",
        "print(embedding(text_vectorization([df_cleaned.Content[0]])))\n",
        "print('Vector size : ', embedding(text_vectorization([df_cleaned.Content[0]])).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THK5fgorcSFo"
      },
      "outputs": [],
      "source": [
        "def create_tf_datasets(X, y, batch_size=32, buffer_size=None, cache=False):\n",
        "    # Convert to tf.data.Dataset\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "    # Apply prefetching to dataset\n",
        "    if buffer_size is not None:\n",
        "        ds = ds.shuffle(buffer_size=buffer_size, seed=3)\n",
        "\n",
        "    ds = ds.batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    # Apply caching to the dataset if cache is set to True\n",
        "    if cache:\n",
        "        ds = ds.cache()\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8djZjXQcSFo"
      },
      "source": [
        "`buffer_size` in TensorFlow, particularly in operations like `tf.data.Dataset.shuffle(buffer_size)`, refers to the number of elements from a dataset that are randomly sampled and placed in a buffer. This ensures randomness when forming batches or reading data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTQu97xucSFo"
      },
      "outputs": [],
      "source": [
        "# buffer_size= total_samples\n",
        "# train_dataset = create_tf_datasets(X_train_vec, y_train,\n",
        "#                                    batch_size=32, buffer_size=None)\n",
        "# val_dataset = create_tf_datasets(X_val_vec, y_val,\n",
        "#                                    batch_size=32, buffer_size=None)\n",
        "# test_dataset = create_tf_datasets(X_test_vec, y_test,\n",
        "#                                   batch_size=32, buffer_size=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q652P2BGcSFo"
      },
      "source": [
        "## 6.1. Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYf779ISaZFC",
        "outputId": "2cb73edf-0491-449c-a1f1-49da8e5e99a1"
      },
      "outputs": [],
      "source": [
        "y_train_ohe = to_categorical(y_train)\n",
        "y_val_ohe = to_categorical(y_val)\n",
        "y_test_ohe = to_categorical(y_test)\n",
        "y_train_ohe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VcPaIMlrGTv"
      },
      "outputs": [],
      "source": [
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),  # Classes are already 0 and 1\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "# Create a dictionary mapping integer labels to their weights\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Create sample weights for training data\n",
        "sample_weights = np.array([class_weights_dict[label] for label in y_train])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LonyVAPbaNYV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, GRU, Dropout, Reshape\n",
        "\n",
        "## Clear Session\n",
        "seed = 20\n",
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "## Define the architecture\n",
        "model_lstm_1 = Sequential()\n",
        "model_lstm_1.add(text_vectorization)\n",
        "model_lstm_1.add(embedding)\n",
        "model_lstm_1.add(LSTM(32, return_sequences=True, kernel_initializer=tf.keras.initializers.GlorotUniform(seed)))\n",
        "model_lstm_1.add(Dropout(0.1))\n",
        "model_lstm_1.add(LSTM(16, kernel_initializer=tf.keras.initializers.GlorotUniform(seed)))\n",
        "model_lstm_1.add(Dropout(0.1))\n",
        "model_lstm_1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_lstm_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqQxeU7RcSFo"
      },
      "source": [
        "## 6.2. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zGIuweT_n0h",
        "outputId": "3dd0edee-2ca4-4b81-bad7-a6a84d5fe06b"
      },
      "outputs": [],
      "source": [
        "model_lstm_1_hist = model_lstm_1.fit(X_train, y_train, epochs=25, validation_data=(X_val, y_val), batch_size=128, sample_weight=sample_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gJcMQFkcSFp"
      },
      "source": [
        "## 6.3. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "jNZ_lckL4G7k",
        "outputId": "7e3e8eca-9f82-445c-cbc7-dc16f34879aa"
      },
      "outputs": [],
      "source": [
        "# Plot Training Results\n",
        "\n",
        "model_lstm_1_hist_df = pd.DataFrame(model_lstm_1_hist.history)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.lineplot(data=model_lstm_1_hist_df[['accuracy', 'val_accuracy']])\n",
        "plt.grid()\n",
        "plt.title('Accuracy vs Val-Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.lineplot(data=model_lstm_1_hist_df[['loss', 'val_loss']])\n",
        "plt.grid()\n",
        "plt.title('Loss vs Val-Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3niMKibYa7I"
      },
      "outputs": [],
      "source": [
        "# Defining a function to make predictions and evaluate the model\n",
        "def predict_and_evaluate(model, X,y, threshold=0.5,\n",
        "                         target_names: list = None):\n",
        "\n",
        "    y_true = []  # True labels\n",
        "    y_pred = []  # Predicted labels\n",
        "    y_pred_proba= []\n",
        "\n",
        "    # Iterating through the dataset to get true labels and predictions\n",
        "    predict_proba = model.predict(X, verbose=0)\n",
        "\n",
        "    # Using tf.where to determine labels based on the threshold\n",
        "    predictions = tf.where(predict_proba >= threshold, 1, 0)\n",
        "\n",
        "    y_pred_proba.extend(predict_proba)\n",
        "    y_pred.extend(predictions.numpy().flatten())\n",
        "\n",
        "    # Iterating through the dataset to get true labels and predictions\n",
        "\n",
        "    y_true.extend(y.array)\n",
        "\n",
        "\n",
        "    y_pred_proba = np.array(y_pred_proba)\n",
        "    y_pred_proba = y_pred_proba.flatten()\n",
        "    cr = classification_report(y_true, y_pred, target_names=target_names)\n",
        "    return cr,y_true, y_pred, y_pred_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggu4erP3YcfH",
        "outputId": "0b30cb80-032c-4048-9a83-b9f1ba1a4bc7"
      },
      "outputs": [],
      "source": [
        "cr_result, y_true, y_pred,y_pred_proba = predict_and_evaluate(model_lstm_1, X_test, y_test,\n",
        "                                                              target_names=['Non-Hate', 'Hate Speech'])\n",
        "print(cr_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ3t4vONcSFp"
      },
      "source": [
        "# 7. ANN Improvement (Sequential API/Functional API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOFX1OvWMZrD"
      },
      "outputs": [],
      "source": [
        "callbacks_1 = [\n",
        "    EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath='model_weight1.weights.h5', monitor='val_loss',mode='min',\n",
        "                    save_weights_only=True, save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLJ5O8mDcSFp"
      },
      "source": [
        "## 7.1. Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzhEOuShkE23"
      },
      "source": [
        "### LSTM with Bidirectional LSTM and Other Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYrbZmWqkL2b"
      },
      "outputs": [],
      "source": [
        "# Model Training using LSTM with Transfer Learning\n",
        "# %%time\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, GRU, Dropout, Reshape\n",
        "\n",
        "## Clear Session\n",
        "seed = 20\n",
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "## Define the architecture\n",
        "model_lstm_3 = Sequential()\n",
        "model_lstm_3.add(text_vectorization)\n",
        "model_lstm_3.add(embedding)\n",
        "model_lstm_3.add(Bidirectional(LSTM(64, return_sequences=True, kernel_initializer=tf.keras.initializers.GlorotUniform(seed))))\n",
        "model_lstm_3.add(Dropout(0.01))\n",
        "model_lstm_3.add(Bidirectional(LSTM(32, kernel_initializer=tf.keras.initializers.GlorotUniform(seed))))\n",
        "model_lstm_3.add(Dropout(0.1))\n",
        "model_lstm_3.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_lstm_3.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001), metrics='accuracy') #tf.keras.metrics.Precision(name='precision')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bELGKE_ncSFp"
      },
      "source": [
        "## 7.2. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyKTI4kskkxr",
        "outputId": "93b70c5f-7475-4d65-843c-556c755d9cd7"
      },
      "outputs": [],
      "source": [
        "model_lstm_3_hist = model_lstm_3.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), batch_size=32, callbacks=callbacks_1, sample_weight=sample_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjzPu4xCcSFp"
      },
      "source": [
        "## 7.3. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "Z2s3aGCJnonx",
        "outputId": "417cc57c-6974-4126-927a-2585c8b13af7"
      },
      "outputs": [],
      "source": [
        "# Plot Training Results\n",
        "\n",
        "model_lstm_3_hist_df = pd.DataFrame(model_lstm_3_hist.history)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.lineplot(data=model_lstm_3_hist_df[['accuracy', 'val_accuracy']])\n",
        "plt.grid()\n",
        "plt.title('Recall vs Val-Recall')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.lineplot(data=model_lstm_3_hist_df[['loss', 'val_loss']])\n",
        "plt.grid()\n",
        "plt.title('Loss vs Val-Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtLLF_Mgny7H",
        "outputId": "fa6f358d-b565-4483-f5b0-3da3924087ed"
      },
      "outputs": [],
      "source": [
        "cr_result, y_true, y_pred,y_pred_proba = predict_and_evaluate(model_lstm_3, X_test, y_test,\n",
        "                                                              target_names=['Non-Hate', 'Hate Speech'])\n",
        "print(cr_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCfRD9j4PETR",
        "outputId": "13a68ba8-5cfd-4d32-a5d3-d33df0cdca3e"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model_lstm_3.evaluate(X_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekTxDmuYcSFp"
      },
      "source": [
        "# 8. Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTGM_gj8Seis"
      },
      "outputs": [],
      "source": [
        "#save model\n",
        "model_lstm_3.save('model_lstm_3.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGZakr4bcSFp"
      },
      "source": [
        "# 9. Model Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8fPykQBcSFp"
      },
      "source": [
        "# 10. Pengambilan Kesimpulan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDSAR1hKjK1J"
      },
      "source": [
        "## Model Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RimSVqVjExJ"
      },
      "source": [
        "**Base Model Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBwxmL7bjDkM",
        "outputId": "28a3aab7-fc2f-4def-b1bd-558f166b12c8"
      },
      "outputs": [],
      "source": [
        "cr_result, y_true, y_pred,y_pred_proba = predict_and_evaluate(model_lstm_1, X_test, y_test,\n",
        "                                                              target_names=['Non-Hate', 'Hate Speech'])\n",
        "print(cr_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3M7Ql3bjdd8"
      },
      "source": [
        "**Base Model Performance - Evaluation**\n",
        "\n",
        "From this report, we can see that:\n",
        "\n",
        "* On predicting Non-hate data, we can see that it has good precision, recall, and f1-score. Implying that the model can predict Non-Hate Speech Social Media Content with ease.\n",
        "* However, we can see that the base model has good accuracy because it can only predict Non-hate Speech, but it doesn't have prediction performance based based on 0 score on precision, recall, anf f1-score of Hate Speech prediction score.\n",
        "* Factor that might influence this low performance is because the imbalance in this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGBAIZmNmVZT"
      },
      "source": [
        "**Improved Model Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iMVuMhKmav9",
        "outputId": "8e21e0e8-f0e7-4f11-a611-e0a638bcffd0"
      },
      "outputs": [],
      "source": [
        "cr_result, y_true, y_pred,y_pred_proba = predict_and_evaluate(model_lstm_3, X_test, y_test,\n",
        "                                                              target_names=['Non-Hate', 'Hate Speech'])\n",
        "print(cr_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrztHJ_zm7QX"
      },
      "source": [
        "**Improved Model Performance - Evaluation**\n",
        "\n",
        "From this report, we can see that:\n",
        "\n",
        "* The precision score on non-hate speech prediction is increased, while the recall and f1-score is is slightly decreased.\n",
        "\n",
        "* On the other hand the Hate Speech prediction precision, recall, and f1-score increase, with the cost of decreased accuracy.\n",
        "\n",
        "* This implies that tuning on this model increase this model Hate Speech predicting performance. Unfortunately, the performance is still not high enough to accurately predict hate speech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ6e0TASuKyR"
      },
      "source": [
        "**Advantages And Disadvantages of using Improved Model**\n",
        "\n",
        "**Advantages**\n",
        "* **Improved Performance than Base Model**: Tuning the base ANN model improve the performance issue on Hate-speech prediction due to imbalance dataset.\n",
        "\n",
        "**Disadvantages**\n",
        "* **Subpar Performance Overall**: Predicting performance for Hate-Speech in overall is still subpar on precision, recall, and f1-score. This implies that the model can't be used as it will tend to mispredict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyHD_dT11Zhu"
      },
      "source": [
        "**Model Analysis Conclusion**\n",
        "\n",
        "* That Improved model has improved performance compared to base model in terms of prediction performance on Hate-Speech Content in social media.\n",
        "\n",
        "* However, the overall performance for improved model still results in subpar performance in correctly predicting Hate-speech Content. Using this model might giving the risk of misprediction on sentiment that will increase the workload or render the prediction result useless for project that will use this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQuo8wlu2p2Q"
      },
      "source": [
        "## Business Insight & Further Experiments Recommendation\n",
        "\n",
        "**Business Insights**\n",
        "\n",
        "* Sentiment analysis on hate speech content in social media might be good to analyze public view on some instances or person such as country, people, or organization. It is necessary to look out for any harsh word or mockery that might indicate hate speech.\n",
        "\n",
        "* Some mockery and terms that tends to racism are quite high in this dataset. This represents that social media company needs to look our for this racism term to further determine whether the some post is hate speech or not. Other company from other industries also need to watch out not to post or release any product that are having risk to be called out as racism, as it might increase the hate speech on the companie's social media post, hurting the reputation.\n",
        "\n",
        "**Further Experiments Recommendation**\n",
        "\n",
        "* **Balancing dataset**: Undersampling or finding other dataset that are more balanced will increase the prediction performance ot the model.\n",
        "* **Relabeling**: We found that some contents in this dataset are mislabeled. It is better to reevaluate all label for content in this dataset to provide accurate dataset to be trained to model.\n",
        "* **Increase Computational Power**: On this project, we only have some handful computational power, which increase the time cost for training model. By increasing the computational power, it is possible to train model with more data to increase it's accuracy and performance.\n",
        "* **Explore tuning possibility**: Due to time constraint, we can't explore further of many tuning possibility. But it is possible that tuning more might also increase the score from classification report.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82yNeHrv8qAK"
      },
      "source": [
        "## Overall Analysis\n",
        "\n",
        "This analysis shows that Improved model are a better model than based model that we've tested. After we tune the base model, we can see some improvement on Hate speech prediction. However the overall performance of this model cannot be implemented to project such as hate speech censoring or sentiment analysis for product or project for its overall subpar performance. And so it is better to further experiment on this project buy evaluating the dataset on mislabeling error and imbalanced data phenomenon. It is highly advisable to increase the computational cost to decrease time cost or possibility to train the model with bigger dataset to improve its prediction performance. Also by experimenting more on model tuning, it will also increase the performance of model to satisfaction level, which will accurately predict sentiment from the social media content whether it is a hate speech or not."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
